View(ModelData)
#Tree Model
ModelData = finalData[101:nrow(finalData),3:ncol(finalData)]
ModelData[,FTGrDiff := FTGrAvg - FTGrAvgAway]
ModelData[,STrDiff := STrAvg - STrAvgAway]
ModelData[,FoulsrDiff := FoulsrAvg - FoulsrAvgAway]
ModelData[,CornersrDiff := CornersrAvg - CornersrAvgAway]
ModelData[,YellowrDiff := YellowrAvg - YellowrAvgAway]
ModelData[,xgrDiff := xgrAvg - xgrAvgAway]
ModelData[,nsxgrDiff := nsxgrAvg - nsxgrAvgAway]
ModelData[,adj_scorerDiff := adj_scorerAvg - adj_scorerAvgAway]
ModelData[,spiDiff := spi1 - spi2]
ModelData[,proj_scoreDiff := proj_score1 - proj_score2]
ModelData[,attackDiff := attackrAvg - attackrAvgAway]
ModelData[,InefficiencyDiff := InefficiencyrAvg - InefficiencyrAvgAway]
#Remove unneeded cols
ModelData[,5:30:=NULL]
toRemove = c("proj_score1","proj_score2")
ModelData[,(toRemove):=NULL]
View(ModelData)
#Variable Analysis
corrplot(cor(ModelData[,2:ncol(ModelData)]), method ="color",order = "AOE")
#Classification Tree
EPLTree <- rpart(FTR~. , data= ModelData, method = "class")
fancyRpartPlot(EPLTree)
#Predicion - All data
predictFTR = predict(EPLTree, type = "class")
table(predictFTR, ModelData$FTR)
library(caTools)
set.seed(3000)
spl <- sample.split(ModelData$FTR, SplitRatio = 0.7)
Train <- subset(ModelData, spl == TRUE)
Test <- subset(ModelData, spl == FALSE)
#Cross Validation
numFolds <- trainControl(method="cv", number=5)
cpGrid <- expand.grid(.cp=seq(0.001,0.05,0.001))#cp paramaeters to test as numbers from 0.0005 to 0.05, in increments of 0.01.
train(FTR~BbAvH+ BbAvD + prob1 + prob2 +attack_diff+STrDiff+ CornersrDiff, data = Train, method="rpart", trControl=numFolds, tuneGrid = cpGrid) #cp = 0.0155
train(FTR~., data = Train, method="rpart", trControl=numFolds, tuneGrid = cpGrid) #cp = 0.0155
EPLTree <- rpart(FTR~BbAvH+ BbAvD + prob1 + prob2 +attack_diff+STrDiff+ CornersrDiff, data= Train, method = "class",  cp = 0.013)
#Classification Tree - With Validation Set
EPLTree <- rpart(FTR~., data= Train, method = "class",  cp = 0.013)
fancyRpartPlot(EPLTree)
#Predicion - Test data
predictFTR = predict(EPLTree, type = "class", newdata = Test)
table(predictFTR, Test$FTR)
EPLForest = randomForest(FTR~., data= Train)
#transformedModelData
# ModelData[,BbAvA:=log(BbAvA)]
# ModelData[,BbAvD:=log(BbAvD)]
# ModelData[,BbAvH:=log(BbAvH)]
ModelData[,FTR:= as.factor(FTR)]
ModelData[,2:ncol(ModelData)]%>%gather()%>%ggplot(aes(value))+facet_wrap(~key, scales = "free")+geom_histogram()
EPLForest = randomForest(FTR~., data= Train)
#transformedModelData
# ModelData[,BbAvA:=log(BbAvA)]
# ModelData[,BbAvD:=log(BbAvD)]
# ModelData[,BbAvH:=log(BbAvH)]
ModelData[,FTR:= as.factor(FTR)]
EPLTree <- rpart(FTR~. , data= ModelData, method = "class")
fancyRpartPlot(EPLTree)
printcp(EPLTree)
plotcp(EPLTree)
#Predicion - All data
predictFTR = predict(EPLTree, type = "class")
table(predictFTR, ModelData$FTR)
#Collect Train and Test Data
library(caTools)
set.seed(3000)
spl <- sample.split(ModelData$FTR, SplitRatio = 0.7)
Train <- subset(ModelData, spl == TRUE)
Test <- subset(ModelData, spl == FALSE)
#Cross Validation
numFolds <- trainControl(method="cv", number=5)
cpGrid <- expand.grid(.cp=seq(0.001,0.05,0.001))#cp paramaeters to test as numbers from 0.0005 to 0.05, in increments of 0.01.
train(FTR~., data = Train, method="rpart", trControl=numFolds, tuneGrid = cpGrid) #cp = 0.0155
#Classification Tree - With Validation Set
EPLTree <- rpart(FTR~., data= Train, method = "class",  cp = 0.013)
fancyRpartPlot(EPLTree)
#Predicion - Test data
predictFTR = predict(EPLTree, type = "class", newdata = Test)
table(predictFTR, Test$FTR)
EPLForest = randomForest(FTR~., data= Train)
predictFTRForest = predict(EPLForest, newdata = Test)
table(predictFTRForest, Test$FTR)
EPLForest = randomForest(FTR~., data= Train,ntree = 200 )
predictFTRForest = predict(EPLForest, newdata = Test)
table(predictFTRForest, Test$FTR)
EPLForest = randomForest(FTR~., data= Train,ntree = 200, nodesize = 30 )
predictFTRForest = predict(EPLForest, newdata = Test)
table(predictFTRForest, Test$FTR)
EPLForest = randomForest(FTR~., data= Train,ntree = 200, nodesize = 35 )
predictFTRForest = predict(EPLForest, newdata = Test)
table(predictFTRForest, Test$FTR)
EPLForest = randomForest(FTR~., data= Train,ntree = 300, nodesize = 35 )
predictFTRForest = predict(EPLForest, newdata = Test)
table(predictFTRForest, Test$FTR)
EPLForest = randomForest(FTR~., data= Train,ntree = 400, nodesize = 35 )
predictFTRForest = predict(EPLForest, newdata = Test)
table(predictFTRForest, Test$FTR)
EPLForest = randomForest(FTR~., data= Train,ntree = 200, nodesize = 38 )
predictFTRForest = predict(EPLForest, newdata = Test)
table(predictFTRForest, Test$FTR)
glmModel = multinom(FTR~., data= Train)
summary(glmModel)
predictGlm =predict(glmModel,Test, "class")
predictGlmProbs =predict(glmModel,Test, "probs")
table(predictGlm, Test$FTR)
predictionCompare = as.data.table(cbind(predictGlm, predictGlmProbs, Test$FTR))
ldaModel = lda(FTR~., data = ldaTrain)
ldaPred = predict(ldaModel, newdata = ldaTest)
table(ldaPred$class, ldaTest$FTR)
naiveModel = naiveBayes(FTR~., data = ldaTrain)
naivePred = predict(naiveModel, ldaTest, "class")
table(naivePred, ldaTest$FTR)
ldaTrain = copy(Train)
ldaTrain[,BbAvH:=log(BbAvH)]
ldaTrain[,BbAvD:=log(BbAvD)]
ldaTest = copy(Test)
ldaTest[,BbAvH:=log(BbAvH)]
ldaTest[,BbAvD:=log(BbAvD)]
ldaModel = lda(FTR~., data = ldaTrain)
ldaPred = predict(ldaModel, newdata = ldaTest)
table(ldaPred$class, ldaTest$FTR)
naiveModel = naiveBayes(FTR~., data = ldaTrain)
naivePred = predict(naiveModel, ldaTest, "class")
table(naivePred, ldaTest$FTR)
View(EPL1617)
View(EPL1617_mod)
View(EPL1617_all)
ldaModel = lda(FTR~., data = ldaTrain, CV =TRUE)
ldaPred = predict(ldaModel, newdata = ldaTest)
table(ldaPred$class, ldaTest$FTR)
ldaPred = predict(ldaModel, newdata = ldaTest)
qdaModel = qda(FTR~BbAvH+BbAvD+STrDiff+ xgrDiff+ nsxgrDiff+CornersrDiff, data = ldaTrain)
qdaPred = predict(qdaModel, newdata = ldaTest)
table(qdaPred$class, ldaTest$FTR)
ldaModel = lda(FTR~., data = ldaTrain)
ldaPred = predict(ldaModel, newdata = ldaTest)
qdaModel = qda(FTR~BbAvH+BbAvD+STrDiff+ xgrDiff+ nsxgrDiff+CornersrDiff, data = ldaTrain)
ldaModel = lda(FTR~BbAvH+BbAvD+STrDiff+ xgrDiff+ nsxgrDiff+CornersrDiff, data = ldaTrain)
ldaPred = predict(ldaModel, newdata = ldaTest)
ldaModel = lda(FTR~BbAvH+BbAvD+STrDiff+ xgrDiff+ nsxgrDiff+CornersrDiff, data = ldaTrain, CV=TRUE)
ldaPred = predict(ldaModel, newdata = ldaTest)
ldaModel
table(ldaModel$class, ldaTrain$FTR)
ldaData = rbind(ldaTest,ldaTrain)
ldaModel = lda(FTR~BbAvH+BbAvD+STrDiff+ xgrDiff+ nsxgrDiff+CornersrDiff, data = ldaData, CV=TRUE)
#ldaPred = predict(ldaModel, newdata = ldaTest)
table(ldaModel$class, ldaData$FTR)
EPLForest = randomForest(FTR~., data= Train,ntree = 200, nodesize = 38 , mtry =5)
predictFTRForest = predict(EPLForest, newdata = Test)
table(predictFTRForest, Test$FTR)
library(factoextra)
pcaEPL = prcomp(Train, scale=T)
View(Train)
pcaEPL = prcomp(Train[,-1], scale=T)
fviz_eig(pcaEPL)
fviz_pca_var(pcaEPL,
col.var = "contrib", # Color by contributions to the PC
gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
re
fviz_pca_var(pcaEPL,
col.var = "contrib", # Color by contributions to the PC
gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
repel = TRUE     # Avoid text overlapping
)
pcaEPL = prcomp(ModelData[,-1], scale=T)
fviz_eig(pcaEPL)
fviz_pca_var(pcaEPL,
col.var = "contrib", # Color by contributions to the PC
gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
repel = TRUE     # Avoid text overlapping
)
pcaEPL = prcomp(ModelData[,-1], scale=T)
fviz_eig(pcaEPL)
#Variables
fviz_pca_var(pcaEPL,
col.var = "contrib", # Color by contributions to the PC
gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
repel = TRUE     # Avoid text overlapping
)
#PCA result
eplVariables = get_pca_var(pcaEPL)
#PCA result
eplVariablesPCA = get_pca_var(pcaEPL)
eplVariablesPCA$coord
eplVariablesPCA$contrib
eplVariablesPCA = get_pca_var(pcaEPL)
eplVariablesPCA$coord
eplVariablesPCA$contrib
fviz_pca_var(pcaEPL,
col.var = "contrib", # Color by contributions to the PC
gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
repel = TRUE     # Avoid text overlapping
)
EPLForest = randomForest(FTR~., data= Train,ntree = 200, nodesize = 38 , mtry =5)
predictFTRForest = predict(EPLForest, newdata = Test)
table(predictFTRForest, Test$FTR)
EPLForest = randomForest(FTR~prob2+probtie+BbAvD+BbAvA+spiDiff+attackDiff, data= Train,ntree = 200, nodesize = 38 , mtry =5)
predictFTRForest = predict(EPLForest, newdata = Test)
table(predictFTRForest, Test$FTR)
fviz_pca_var(pcaEPL,
col.var = "contrib", # Color by contributions to the PC
gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
repel = TRUE     # Avoid text overlapping)
)
EPLForest = randomForest(FTR~., data= Train,ntree = 200, nodesize = 38 , mtry =5)
predictFTRForest = predict(EPLForest, newdata = Test)
table(predictFTRForest, Test$FTR)
EPLForest = randomForest(FTR~prob2+probtie+BbAvD+BbAvA+BbAvH+attackDiff+FoulsrDiff+CornersrDiff, data= Train,ntree = 200, nodesize = 38 , mtry =5)
predictFTRForest = predict(EPLForest, newdata = Test)
table(predictFTRForest, Test$FTR)
eplVariablesPCA$contrib
rowsum(eplVariablesPCA$contrib)
dim(eplVariablesPCA$contrib)
rowsum(eplVariablesPCA$contrib[,2:20])
rowSums(eplVariablesPCA$contrib[,2:20])
fviz_eig(pcaEPL)
rowSums(eplVariablesPCA$contrib[,2:10])
sort(rowSums(eplVariablesPCA$contrib[,2:10]))
EPLForest = randomForest(FTR~., data= Train,ntree = 200, nodesize = 38 , mtry =5)
predictFTRForest = predict(EPLForest, newdata = Test)
table(predictFTRForest, Test$FTR)
EPLForest = randomForest(FTR~YellowrDiff, data= Train,ntree = 200, nodesize = 38 , mtry =5)
predictFTRForest = predict(EPLForest, newdata = Test)
table(predictFTRForest, Test$FTR)
EPLForest = randomForest(FTR~., data= Train,ntree = 200, nodesize = 38 , mtry =5)
predictFTRForest = predict(EPLForest, newdata = Test)
table(predictFTRForest, Test$FTR)
EPLForest = randomForest(FTR~YellowrDiff+FoulsrDiff+CornersrDiff+STrDiff, data= Train,ntree = 200, nodesize = 38 , mtry =5)
predictFTRForest = predict(EPLForest, newdata = Test)
table(predictFTRForest, Test$FTR)
EPLForest = randomForest(FTR~., data= Train,ntree = 200, nodesize = 38 , mtry =5)
predictFTRForest = predict(EPLForest, newdata = Test)
table(predictFTRForest, Test$FTR)
EPLForest = randomForest(FTR~YellowrDiff+FoulsrDiff+CornersrDiff+STrDiff, data= Train,ntree = 100, nodesize = 38)
predictFTRForest = predict(EPLForest, newdata = Test)
table(predictFTRForest, Test$FTR)
EPLForest = randomForest(FTR~., data= Train,ntree = 200, nodesize = 38 , mtry =5)
predictFTRForest = predict(EPLForest, newdata = Test)
table(predictFTRForest, Test$FTR)
EPLForest = randomForest(FTR~YellowrDiff+FoulsrDiff+CornersrDiff+STrDiff+BbAvH, data= Train,ntree = 100, nodesize = 38)
predictFTRForest = predict(EPLForest, newdata = Test)
table(predictFTRForest, Test$FTR)
EPLForest = randomForest(FTR~., data= Train,ntree = 200, nodesize = 38 )
predictFTRForest = predict(EPLForest, newdata = Test)
table(predictFTRForest, Test$FTR)
EPLForest = randomForest(FTR~YellowrDiff+FoulsrDiff+CornersrDiff+STrDiff+BbAvH, data= Train,ntree = 200, nodesize = 38)
predictFTRForest = predict(EPLForest, newdata = Test)
table(predictFTRForest, Test$FTR)
numFolds <- trainControl(method="cv", number=5)
cpGrid <- expand.grid(.cp=seq(0.0001,0.05,0.0001))#cp paramaeters to test as numbers from 0.0005 to 0.05, in increments of 0.01.
train(FTR~YellowrDiff+FoulsrDiff+CornersrDiff+STrDiff+BbAvH, data = Train, method="rpart", trControl=numFolds, tuneGrid = cpGrid) #cp = 0.0155
#Classification Tree - With Validation Set
EPLTree <- rpart(FTR~YellowrDiff+FoulsrDiff+CornersrDiff+STrDiff+BbAvH, data= Train, method = "class",  cp = 0.0088)
fancyRpartPlot(EPLTree)
#Predicion - Test data
predictFTR = predict(EPLTree, type = "class", newdata = Test)
table(predictFTR, Test$FTR)
EPLForest = randomForest(FTR~., data= Train,ntree = 200, nodesize = 38 )
predictFTRForest = predict(EPLForest, newdata = Test)
table(predictFTRForest, Test$FTR)
EPLForest = randomForest(FTR~YellowrDiff+FoulsrDiff+CornersrDiff+STrDiff+BbAvH+spi2+spi1+BbAvD+BbAvA+adj_scorerDiff+FTGrDiff, data= Train,ntree = 200, nodesize = 38)
predictFTRForest = predict(EPLForest, newdata = Test)
table(predictFTRForest, Test$FTR)
#Load Required Libraries
library(data.table)
library(reshape2)
library(zoo)
library(corrplot)
library(tidyr)
library(ggplot2)
library(randomForest)
library(caret)
library(e1071)
library(RColorBrewer)
library(rattle)
library(nnet)
library(MASS)
library(rpart)
library(rpart.plot)
library(factoextra)
pcaEPL = prcomp(ModelData[,-1], scale=T)
fviz_eig(pcaEPL)
pcaEPL = prcomp(ModelData[,-1], scale=T)
fviz_eig(pcaEPL)
#Variables
fviz_pca_var(pcaEPL,
col.var = "contrib", # Color by contributions to the PC
gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
repel = TRUE     # Avoid text overlapping
)
#PCA result
eplVariablesPCA = get_pca_var(pcaEPL)
eplVariablesPCA$coord
install.packages("fourfoldplot")
fourfoldplot (table(predictFTR, Test$FTR))
#Tree Model
ModelData = finalData[101:nrow(finalData),3:ncol(finalData)]
ModelData[,FTGrDiff := FTGrAvg - FTGrAvgAway]
ModelData[,STrDiff := STrAvg - STrAvgAway]
ModelData[,FoulsrDiff := FoulsrAvg - FoulsrAvgAway]
ModelData[,CornersrDiff := CornersrAvg - CornersrAvgAway]
ModelData[,YellowrDiff := YellowrAvg - YellowrAvgAway]
ModelData[,xgrDiff := xgrAvg - xgrAvgAway]
ModelData[,nsxgrDiff := nsxgrAvg - nsxgrAvgAway]
ModelData[,adj_scorerDiff := adj_scorerAvg - adj_scorerAvgAway]
ModelData[,spiDiff := spi1 - spi2]
ModelData[,proj_scoreDiff := proj_score1 - proj_score2]
ModelData[,attackDiff := attackrAvg - attackrAvgAway]
ModelData[,InefficiencyDiff := InefficiencyrAvg - InefficiencyrAvgAway]
#Remove unneeded cols
ModelData[,5:30:=NULL]
toRemove = c("proj_score1","proj_score2")
ModelData[,(toRemove):=NULL]
pcaEPL = prcomp(ModelData[,-1], scale=T)
fviz_eig(pcaEPL)
#Variables
fviz_pca_var(pcaEPL,
col.var = "contrib", # Color by contributions to the PC
gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
repel = TRUE     # Avoid text overlapping
)
EPLForest = randomForest(FTR~YellowrDiff+FoulsrDiff+CornersrDiff+STrDiff+BbAvH+spi2+spi1+BbAvD+BbAvA+adj_scorerDiff+FTGrDiff+proj_scoreDiff, data= Train,ntree = 200, nodesize = 38)
predictFTRForest = predict(EPLForest, newdata = Test)
table(predictFTRForest, Test$FTR)
knitr::opts_chunk$set(echo = TRUE, cache = TRUE)
#Load Required Libraries
library(data.table)
library(reshape2)
library(zoo)
library(corrplot)
library(tidyr)
library(ggplot2)
library(randomForest)
library(caret)
library(e1071)
library(RColorBrewer)
library(rattle)
library(nnet)
library(MASS)
library(rpart)
library(rpart.plot)
library(factoextra) #PCA Analysis visualization
#Read Match Stats and Betting data
EPL1819_all = fread("season-1819_csv.csv")
EPL1718_all = fread("season-1718_csv.csv")
EPL1617_all = fread("season-1617_csv.csv")
EPL1920_all = fread("season-1920_csv.csv")
setnames(EPL1920_all,c("AvgA","AvgH","AvgD"), c("BbAvA","BbAvH","BbAvD"))#Fix closing betting averages name changes from the 19/20 dataset
#Seperate Modelling Data that doesn't need to be aggregated
modelCols = c("HomeTeam","AwayTeam","FTR","BbAvH","BbAvD","BbAvA")
EPL1617_mod = EPL1617_all[,..modelCols]
EPL1718_mod = EPL1718_all[,..modelCols]
EPL1819_mod = EPL1819_all[,..modelCols]
EPL1920_mod = EPL1920_all[,..modelCols]
modelData = rbind(EPL1617_mod, EPL1718_mod,EPL1819_mod ,EPL1920_mod)
#Remove Extraneous columns
relevantColumns = c("HomeTeam","AwayTeam","FTHG","FTAG","HTHG","HTAG","HS","AS","HST","AST","HF","AF","HC","AC","HY","AY","HR","AR")
EPL1617 = EPL1617_all[,..relevantColumns]
EPL1718 = EPL1718_all[,..relevantColumns]
EPL1819 = EPL1819_all[,..relevantColumns]
EPL1920 = EPL1920_all[,..relevantColumns]
# Load 538 spi ratings and xG data
spiMatches = fread("spi_matches.csv")
spiMatches = spiMatches[league == "Barclays Premier League",]
#Format spi data to fit stats and betting data - Different team names in both data set
setnames(spiMatches,c("team1","team2"), c("HomeTeam","AwayTeam"))
spiTeamToChange = c("Hull City", "Manchester City", "AFC Bournemouth","Manchester United", "Leicester City", "Stoke City", "Swansea City", "Tottenham Hotspur","West Bromwich Albion","West Ham United", "Brighton and Hove Albion", "Huddersfield", "Wolverhampton", "Cardiff City", "Norwich City")
changeTo = c("Hull","Man City","Bournemouth", "Man United", "Leicester", "Stoke", "Swansea", "Tottenham","West Brom","West Ham","Brighton","Huddersfield","Wolves", "Cardiff", "Norwich")
#Change team names to match
for(i in 1:length(spiTeamToChange)) {
spiMatches[HomeTeam== spiTeamToChange[i],HomeTeam:=changeTo[i]]
spiMatches[AwayTeam== spiTeamToChange[i],AwayTeam:=changeTo[i]]
}
#sort to match order - Betting data is sorted by date, then home team name.
spiMatches[,date := as.Date(date,format="%Y-%m-%d")]
setorder(spiMatches, date, HomeTeam)
#Generate agg cols for spi
spiMatches[,attack1 := xg1+nsxg1+adj_score1]
spiMatches[,attack2 := xg2+nsxg2+adj_score2]
#Column bind spi data and betting data
EPL1617 = cbind(EPL1617, spiMatches[1:380,17:24])
EPL1718 = cbind(EPL1718, spiMatches[381:760,17:24])
EPL1819 = cbind(EPL1819, spiMatches[761:1140,17:24])
EPL1920 = cbind(EPL1920, spiMatches[1141:(1140+nrow(EPL1920)),17:24])
#Efficiency
EPL1617[, Inefficiency1 := attack1-FTHG]
EPL1617[, Inefficiency2 := attack2-FTAG]
EPL1718[, Inefficiency1 := attack1-FTHG]
EPL1718[, Inefficiency2 := attack2-FTAG]
EPL1819[, Inefficiency1 := attack1-FTHG]
EPL1819[, Inefficiency2 := attack2-FTAG]
EPL1920[, Inefficiency1 := attack1-FTHG]
EPL1920[, Inefficiency2 := attack2-FTAG]
allSeasons = list(EPL1617,EPL1718,EPL1819,EPL1920)
#List to hold all tables that have the new columns
meltedDataList = list()
#Separate Predictor variables from Aggregators.
aggregateEplCols = c("HomeTeam","AwayTeam","FTHG","FTAG","HTHG","HTAG","HS","AS","HST","AST","HF","AF","HC","AC","HY","AY","HR","AR")
seasonMeltMeasureList = list(c("HomeTeam", "AwayTeam"),c("FTHG", "FTAG"),c("HTHG", "HTAG"),c("HS", "AS"),c("HST", "AST"),c("HF", "AF"),c("HC", "AC"),c("HY", "AY"),c("HR", "AR"), c("xg1","xg2"),c("nsxg1","nsxg2"),c("adj_score1","adj_score2"),c("attack1","attack2"),c("Inefficiency1","Inefficiency2"))
seasonMeltNames = c("Team","FTG","HTG","Shots","ST","Fouls","Corners","Yellow","Red","xg","nsxg","adj_score","attack","Inefficiency")
#For loop to aggregate each season data
for(season in allSeasons){
#Melt to combine home and away results - Note separate running averages in future
season = melt(season, measure = seasonMeltMeasureList, value.name = seasonMeltNames)
meltedDataList[[length(meltedDataList)+1]] <- season
}
allMeltedData = rbindlist(meltedDataList)
#New columns for the rolling Averages
rollingAvgColumns = paste0(c("FTG","HTG","Shots","ST","Fouls","Corners","Yellow","Red","xg","nsxg","adj_score","attack","Inefficiency"),"rAvg")
##Custom function - Find the rolling mean and then shift by 1, so that the previous averages are used
shift_froll = function(x, n){sh=ift(frollmean(x, n= n))}
#bMelted[,variable:= as.factor(variable)]
#Rolling Maean and Shift
allMeltedData[, (rollingAvgColumns):= lapply(.SD, shift_froll,  n = 5), by = c("Team","variable"), .SDcols =  c("FTG","HTG","Shots","ST","Fouls","Corners","Yellow","Red","xg","nsxg","adj_score","attack","Inefficiency")]
#Generate agg cols for spi
spiMatches[,attack1 := xg1+nsxg1+adj_score1]
spiMatches[,attack2 := xg2+nsxg2+adj_score2]
#Column bind spi data and betting data
EPL1617 = cbind(EPL1617, spiMatches[1:380,17:24])
EPL1718 = cbind(EPL1718, spiMatches[381:760,17:24])
EPL1819 = cbind(EPL1819, spiMatches[761:1140,17:24])
EPL1920 = cbind(EPL1920, spiMatches[1141:(1140+nrow(EPL1920)),17:24])
#Efficiency
EPL1617[, Inefficiency1 := attack1-FTHG]
EPL1617[, Inefficiency2 := attack2-FTAG]
EPL1718[, Inefficiency1 := attack1-FTHG]
EPL1718[, Inefficiency2 := attack2-FTAG]
EPL1819[, Inefficiency1 := attack1-FTHG]
EPL1819[, Inefficiency2 := attack2-FTAG]
EPL1920[, Inefficiency1 := attack1-FTHG]
EPL1920[, Inefficiency2 := attack2-FTAG]
allSeasons = list(EPL1617,EPL1718,EPL1819,EPL1920)
#List to hold all tables that have the new columns
meltedDataList = list()
#Separate Predictor variables from Aggregators.
aggregateEplCols = c("HomeTeam","AwayTeam","FTHG","FTAG","HTHG","HTAG","HS","AS","HST","AST","HF","AF","HC","AC","HY","AY","HR","AR")
seasonMeltMeasureList = list(c("HomeTeam", "AwayTeam"),c("FTHG", "FTAG"),c("HTHG", "HTAG"),c("HS", "AS"),c("HST", "AST"),c("HF", "AF"),c("HC", "AC"),c("HY", "AY"),c("HR", "AR"), c("xg1","xg2"),c("nsxg1","nsxg2"),c("adj_score1","adj_score2"),c("attack1","attack2"),c("Inefficiency1","Inefficiency2"))
seasonMeltNames = c("Team","FTG","HTG","Shots","ST","Fouls","Corners","Yellow","Red","xg","nsxg","adj_score","attack","Inefficiency")
#For loop to aggregate each season data
for(season in allSeasons){
#Melt to combine home and away results - Note separate running averages in future
season = melt(season, measure = seasonMeltMeasureList, value.name = seasonMeltNames)
meltedDataList[[length(meltedDataList)+1]] <- season
}
allMeltedData = rbindlist(meltedDataList)
#New columns for the rolling Averages
rollingAvgColumns = paste0(c("FTG","HTG","Shots","ST","Fouls","Corners","Yellow","Red","xg","nsxg","adj_score","attack","Inefficiency"),"rAvg")
##Custom function - Find the rolling mean and then shift by 1, so that the previous averages are used
shift_froll = function(x, n){shift(frollmean(x, n= n))}
#bMelted[,variable:= as.factor(variable)]
#Rolling Maean and Shift
allMeltedData[, (rollingAvgColumns):= lapply(.SD, shift_froll,  n = 5), by = c("Team","variable"), .SDcols =  c("FTG","HTG","Shots","ST","Fouls","Corners","Yellow","Red","xg","nsxg","adj_score","attack","Inefficiency")]
#Fill NA values from rolling window and shifting with mean
allMeltedData[,16:28 := na.aggregate(allMeltedData[,16:28] )]
#United = allMeltedData[Team == "Man United" & variable == 1,]
#Modelling Data
#--------------
#Fold data into Home and Away
awayNames = names(allMeltedData[,16:28])#Get table names except variable
awayNames = paste0(awayNames,"Away")#Add away to specify stats
allHome = allMeltedData[variable == 1, 16:28] #Home Stats without variable
allAway = allMeltedData[variable == 2, 16:28] #Away stats without the variable
names(allAway) = awayNames
allBound = cbind(allHome, allAway)
#Column bind data to get home and away wide table
finalData = cbind(modelData, allBound,spiMatches[1:nrow(allBound),6:12])
#Tree Model
ModelData = finalData[101:nrow(finalData),3:ncol(finalData)]
#Find difference in stats by team
ModelData[,FTGrDiff := FTGrAvg - FTGrAvgAway]
ModelData[,STrDiff := STrAvg - STrAvgAway]
ModelData[,FoulsrDiff := FoulsrAvg - FoulsrAvgAway]
ModelData[,CornersrDiff := CornersrAvg - CornersrAvgAway]
knitr::opts_chunk$set(echo = TRUE, cache = TRUE)
library(data.table)
library(reshape2)
library(zoo)
library(corrplot)
library(tidyr)
library(ggplot2)
library(randomForest)
library(caret)
library(e1071)
library(RColorBrewer)
library(rattle)
library(nnet)
library(MASS)
library(rpart)
library(rpart.plot)
library(factoextra) #PCA Analysis visualization
library(yardstick)
install.packages("dplyr")
sessionInfo()
update.packages()
knitr::opts_chunk$set(echo = TRUE, cache = TRUE)
#Load Required Libraries
library(data.table)
library(reshape2)
library(zoo)
library(corrplot)
library(tidyr)
sessionInfo()
install.packages("rlang")
knitr::opts_chunk$set(echo = TRUE, cache = TRUE)
library(tidyr)
update.packages()
sessionInfo()
library(rlang)
sessionInfo()
library(tidyr)
